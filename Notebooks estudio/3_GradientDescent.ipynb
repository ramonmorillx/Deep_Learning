{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a631529-dccb-4592-bf0f-8e0e19eb6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a8aeb96-522c-468e-96f7-16448f2c6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f80ea6b8950>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## =============== ##\n",
    "## Define our data ##\n",
    "## =============== ##\n",
    "\n",
    "# input to our model. Represents time in seconds\n",
    "x_data = np.array([0,1,2]).reshape(3,1)\n",
    "# outputs associated to each input. Represents cantidad de lluvia in mm^3\n",
    "t_data = np.array([0.2,1.3,2.4]).reshape(3,1)\n",
    "\n",
    "## display\n",
    "plt.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "plt.xlabel('tiempo')\n",
    "plt.ylabel('cantidad de lluvia')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c53fd02e-5b53-4d01-b40d-accc47c79bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================= ##\n",
    "## ======== functionality for computational graph ======== ##\n",
    "## ======================================================= ##\n",
    "\n",
    "## function implementing an activation function\n",
    "def activation_function_linear(x):\n",
    "    return x\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear(x,w,b):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_linear(np.matmul(x,w) + b)\n",
    "    return y\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear_just_weight(x,w):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation, with no weight'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_linear(np.matmul(x,w))\n",
    "    return y\n",
    "\n",
    "## function that initializes the values of a computational graph\n",
    "def create_computation_graph_linear(n_in,n_out):\n",
    "    ''' Create elements of the computational graph'''\n",
    "    # parameters\n",
    "    w = np.random.randn(n_in,n_out) + 1 # get a random value from standard normal distribution\n",
    "    b = np.random.randn(n_out,)*5 # get a random value from Gaussian with mean 0 and standard deviation 5.\n",
    "\n",
    "    return w,b\n",
    "\n",
    "## function implementing squared loss function\n",
    "def squared_loss_function_just_weight(x,t,w):\n",
    "    y_pred = activation_function_linear(np.matmul(x,w))\n",
    "    return (y_pred-t)**2\n",
    "\n",
    "def grad_squared_loss_just_weight(x,t,w):\n",
    "    # forward operation\n",
    "    y_pred = activation_function_linear(np.matmul(x,w))\n",
    "    \n",
    "    # backward operation (compute gradients / backpropagation / reverse mode autodiff)\n",
    "    grad_w = np.sum(2*(y_pred-t)*x, axis = 0, keepdims = True)\n",
    "    \n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed78ba-ebd0-410d-9503-3fcacb6919fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====================================== ##\n",
    "## ========== Gradient Descent ========== ##\n",
    "## ====================================== ##\n",
    "\n",
    "## number of points in the domain used to plot the functions \n",
    "N_points_domain = 100\n",
    "x_range = np.linspace(-1,4, N_points_domain).reshape((N_points_domain,1))\n",
    "\n",
    "## specify our computational graph\n",
    "n_in = 1\n",
    "n_out = 1\n",
    "\n",
    "## first of all draw loss function against a set of parameters\n",
    "w_range = np.linspace(-10,15,500).reshape((500,n_in,n_out))\n",
    "\n",
    "loss_range = squared_loss_function_just_weight(x_data,t_data,w_range)\n",
    "\n",
    "## accumulate loss per datapoint\n",
    "loss_acc_range = np.sum(loss_range, axis = 1)\n",
    "\n",
    "## squeeze and display\n",
    "loss_acc_range = np.squeeze(loss_acc_range)\n",
    "w_range = np.squeeze(w_range)\n",
    "\n",
    "# plot grid\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "# display loss function\n",
    "ax1.plot(w_range, loss_acc_range, color = 'C0')\n",
    "ax1.set_xlabel('Weight')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.array([9]).reshape(n_in,n_out)\n",
    "\n",
    "## gradient descent parameters\n",
    "lr = 0.025 # try 0.1, 0.01, 0.15, 0.21 to show: fast convergence, slow convergence, convergence with bumping, divergence\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    ## forward plus backward\n",
    "    grad_w = grad_squared_loss_just_weight(x_data,t_data,w)\n",
    "\n",
    "    ## compute function at current parameter value\n",
    "    function = computation_graph_linear_just_weight(x_range, w)\n",
    "\n",
    "    ## compute predictions at current parameter value\n",
    "    y_pred = computation_graph_linear_just_weight(x_data, w)\n",
    "\n",
    "    ## compute loss at current parameter value\n",
    "    loss = squared_loss_function_just_weight(x_data,t_data,w)\n",
    "    loss_acc = np.sum(loss)\n",
    "\n",
    "    ## get the gradient function at the point w (tangent at the point)\n",
    "    gradient_function_w_at_current_w = grad_w * w_range + loss_acc - grad_w * w\n",
    "\n",
    "    ## ============= ##\n",
    "    ## ============= ##\n",
    "    ## START DRAWING ##\n",
    "    ## ============= ##\n",
    "    ## ============= ##\n",
    "    # Clear previous data\n",
    "    ax1.clear()  \n",
    "    ax2.clear()\n",
    "    \n",
    "    w_plot = np.squeeze(w)\n",
    "    grad_w_plot = np.squeeze(grad_w)\n",
    "    x_data_plot = np.squeeze(x_data)\n",
    "    t_data_plot = np.squeeze(t_data)\n",
    "    y_pred_plot = np.squeeze(y_pred)\n",
    "    loss_plot = np.squeeze(loss)\n",
    "    \n",
    "    # get new weight after grad descent. Just for illustration purposes, the real step is done at the end of the loop\n",
    "    w_new_plot = np.squeeze(w-lr*grad_w)\n",
    "\n",
    "    ## ================ ##\n",
    "    ## function picture ##\n",
    "    ax2.plot(x_range,function, color = 'C1', label = 'function: y = w*x')\n",
    "    ax2.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "\n",
    "    ## plot squared loss associated at each point and draw line between dots to highliht what the loss measures\n",
    "    for idx, (xi, ti, yi, sl) in enumerate(zip(x_data_plot,t_data_plot,y_pred_plot,loss_plot)):\n",
    "        if idx == 0:\n",
    "            ax2.plot(xi,yi, 'x', color = 'C1', label = 'network prediction')\n",
    "        else:\n",
    "            ax2.plot(xi,yi, 'x', color = 'C1')\n",
    "        ax2.plot([xi,xi], [ti, yi], '--',color = f\"C1\", alpha = 0.5)\n",
    "        ax2.text(xi, yi, f'{sl:.2f}', fontsize=12, va='top', color = f\"C1\" ) \n",
    "\n",
    "    # label function with the weight at that moment\n",
    "    ax2.text(x_range[-20],function[-20], f'w = {w_plot}', color = 'k', fontsize = 12)\n",
    "    \n",
    "    ax2.text(1, 33, f\"Iteration {e}, squared loss = {loss_acc:.2f}\", fontsize=12, va='bottom', color = f\"C1\" ) \n",
    "    ax2.set_xlabel('tiempo')\n",
    "    ax2.set_ylabel('cantidad de lluvia')\n",
    "    ax2.set_ylim([-10,30])\n",
    "    ax2.legend()\n",
    "\n",
    "    ## draw\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    \n",
    "    ## ===================== ##\n",
    "    ## loss function picture ##\n",
    "    \n",
    "    ## 0. label and axis limits\n",
    "    ax1.set_xlabel('Weight')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_ylim([-200,900])\n",
    "\n",
    "    ## 1. display loss function\n",
    "    ax1.plot(w_range, loss_acc_range, color = 'C0', label = 'loss', zorder = 20)\n",
    "    \n",
    "    ## 2. display current weight\n",
    "    ax1.plot(w_plot, -190, '*', color = 'C1', label = 'current weight', zorder = 50, markersize = 10)\n",
    "    ax1.text(w_plot + 1, -190 , f\"w = {w_plot:.2f}\", fontsize=12, va='bottom', color = f\"C1\" , zorder = 50)\n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(2)\n",
    "\n",
    "    ## animation by drawing horizontal lines on current parameter and updated parameter values\n",
    "    ax1.vlines(np.squeeze(w), ymin=-200, ymax=loss_acc, color='k', linestyles='dotted', zorder = -50)\n",
    "\n",
    "    ## 3. display current loss\n",
    "    ax1.plot(w_plot, loss_acc, 'o', color = 'C0', label = 'loss at current weight', zorder = 20)\n",
    "    ax1.text(w_plot + 0.5, loss_acc , f\"loss = {loss_acc:.2f}\", fontsize=12, va='bottom', color = \"C0\" , zorder = 50)\n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    ## 4. display the gradient function\n",
    "    ax1.plot(w_range, np.squeeze(gradient_function_w_at_current_w), color = 'C2', label = 'gradient function: f(w) = grad_w * w + loss - grad_w * w', zorder = 20)\n",
    "    ax1.text(w_range[-1], np.squeeze(gradient_function_w_at_current_w)[-1], f\"grad_w = {grad_w_plot:.2f}\", fontsize=12, va='bottom', color = f\"C2\" , zorder = 200) \n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(2)\n",
    "\n",
    "    ## draw rest of lines to show update\n",
    "    ax1.hlines(y = loss_acc, xmin=w_new_plot, xmax=w_plot, color='k', linestyles='dotted', zorder = -50)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    ax1.vlines(w_new_plot, ymin=-200, ymax=loss_acc, color='k', linestyles='dotted', zorder = -50)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    ## 5. display new weight\n",
    "    ax1.plot(w_new_plot, -190, '*', color = 'C3', label = 'updated weight: w_new = w - lr*grad_w', zorder = 200, markersize = 10)\n",
    "    ax1.text(w_new_plot, -160, f\"w_new = {w_plot:.2f} -{lr:.2f}*{grad_w_plot:.2f} = {w_plot-lr*grad_w_plot:.2f}\", fontsize=12, va='bottom', color = f\"C3\" , zorder = 200) \n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(2)\n",
    "    \n",
    "\n",
    "\n",
    "    ## wait to see\n",
    "    time.sleep(3)\n",
    "\n",
    "    ## update parameter with gradient descent, for the next update\n",
    "    w = w-lr*grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11adf6da-e445-4afd-94af-de6f4adaf3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c925e1-5658-4237-ab8a-2bc061b0c5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
